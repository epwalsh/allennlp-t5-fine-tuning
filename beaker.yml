version: v2-alpha
description: Train T5 11B
tasks:
  - name: train
    image:
      docker: epwalsh/allennlp-t5:latest
    envVars:
      # Hack to get more shared memory (needed for multiprocess data loading)
      - name: BEAKER_FEATURE_SHARED_MEMORY_OVERRIDE
        value: "true"
      - name: WANDB_API_KEY
        value: "31595ba4a39d7264e056d607541de3fb1ba1651d"
    datasets:
      - mountPath: /data/t5-11b-weights
        source:
          beaker: ds_rcnp05h59beo
      # - mountPath: /data/CNN-DM
      #   source:
      #     beaker: ds_2kde4g0vee2e
    result:
      path: /output
    resources:
      # TODO: change back to 8
      gpuCount: 2
    context:
      cluster: ai2/on-prem-ai2-server
      priority: normal
